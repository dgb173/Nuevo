
INFORME TÉCNICO DEFINITIVO v3 - PROYECTO DE SCRAPING Y ANÁLISIS
======================================================================

**Fecha de Generación:** 25 de septiembre de 2025
**Autor:** Gemini

### 1. RESUMEN EJECUTIVO Y METODOLOGÍA

Este documento es el análisis más exhaustivo del código del proyecto, diseñado para proporcionar una comprensión completa de su arquitectura, flujo de datos y lógica interna a nivel de función.

#### 1.1. Metodología General

El proyecto se fundamenta en las siguientes metodologías clave:

- **Arquitectura de Servidor Web (Flask):** `app.py` actúa como el orquestador central, manejando rutas HTTP, lógica de negocio y la interacción con los módulos de scraping.
- **Scraping Híbrido (Requests + Playwright/Selenium):** Se prioriza el uso de `requests` para peticiones HTTP rápidas y de bajo consumo. Cuando el contenido requiere ejecución de JavaScript, se utiliza un navegador `headless` (sin interfaz gráfica) a través de `Playwright` o `Selenium` como un robusto mecanismo de fallback.
- **Parsing de HTML (BeautifulSoup):** El contenido HTML obtenido se procesa con `BeautifulSoup`, una librería que facilita la navegación y extracción de datos mediante la selección de etiquetas, IDs, clases y otros atributos CSS.
- **Análisis de Datos (Pandas):** Los datos tabulares complejos, como estadísticas de partidos, se cargan en DataFrames de `Pandas` para facilitar su manipulación y análisis.
- **API RESTful y Concurrencia:** La aplicación expone endpoints JSON para una comunicación dinámica con el frontend y utiliza `ThreadPoolExecutor` para ejecutar tareas de scraping en paralelo, mejorando significativamente los tiempos de respuesta en análisis complejos.

--- 

### 2. ANÁLISIS DE CÓDIGO FUENTE - FUNCIÓN POR FUNCIÓN

**NOTA SOBRE LA EXTRACCIÓN HTML:** El análisis de la extracción se basa en los selectores (IDs, clases, etiquetas) presentes en el código Python. Describe qué busca el código en el HTML de la página de origen.

#### 2.1. Archivo: `app.py` (El Orquestador)

(El análisis de este archivo es idéntico al del informe v2, ya que su desglose fue completo. Se puede consultar en `informe_tecnico_detallado_v2.txt`)

#### 2.2. Archivo: `modules/estudio_scraper.py` (El Cerebro del Análisis)

(El análisis de este archivo es idéntico al del informe v2, ya que su desglose fue completo. Se puede consultar en `informe_tecnico_detallado_v2.txt`)

#### 2.3. Archivo: `modules/utils.py` (Utilidades Comunes)

Este módulo agrupa funciones de bajo nivel, reutilizables en todo el proyecto, para parsear y validar datos.

- `get_match_details_from_row_of(...)`
    - **Propósito:** Extraer todos los detalles de un partido desde una única fila `<tr>` de una tabla de historial.
    - **Extracción HTML:** Recibe un objeto de fila de `BeautifulSoup`. Busca en sus celdas `<td>`:
        - **Fecha:** En la 2ª celda, busca un `<span>` con `name="timeData"`.
        - **Equipos:** En la 3ª y 5ª celda, busca el texto dentro de una etiqueta `<a>`.
        - **Resultado:** En la 4ª celda, busca un `<span>` con una clase específica (ej. `fscore_1`) para el marcador.
        - **Hándicap:** En la 12ª celda, lee el atributo `data-o` o el texto de la celda.
    - **Retorno:** Un diccionario con todos los datos limpios.

- `parse_ah_to_number_of(ah_line_str)`
    - **Propósito:** Convierte una línea de hándicap en formato texto (ej. "-0.5/1", "-0.75") a un único valor numérico flotante (ej. -0.75).
    - **Lógica:** Maneja fracciones (como "0.5/1") calculando su promedio. Limpia espacios y convierte comas a puntos.

- `format_ah_as_decimal_string_of(...)`
    - **Propósito:** Formatea un valor de hándicap numérico a un string estandarizado (ej. 0.25, 0.5, 0.75, 1.0).
    - **Lógica:** Aplica redondeo específico para el hándicap asiático a los cuartos de línea más cercanos.

- `check_handicap_cover(...)`
    - **Propósito:** Determina si una apuesta de hándicap habría sido ganadora, perdedora o nula.
    - **Lógica:** Compara el margen de victoria/derrota del equipo favorito con la línea de hándicap para determinar el resultado: "CUBIERTO", "NO CUBIERTO" o "PUSH".

- `check_goal_line_cover(...)`
    - **Propósito:** Determina si el total de goles de un partido fue superior (Over), inferior (Under) o igual (Push) a una línea de goles dada.

- `extract_final_score_of(soup)`
    - **Propósito:** Extraer el marcador final de un partido desde la página de H2H.
    - **Extracción HTML:** Busca un `<div>` con `id="mScore"`. Dentro, busca los `<div>` con clase `score` para obtener los goles de cada equipo.

#### 2.4. Archivo: `modules/analisis_avanzado.py` (Lógica de Análisis Experto)

- `generar_analisis_comparativas_indirectas(data)`
    - **Propósito:** Genera un bloque de texto HTML con una "Nota del Analista" basada en los datos de las comparativas indirectas.
    - **Lógica:** Compara el rendimiento (ataques peligrosos, tiros) con el resultado final para cada equipo en sus partidos de comparativa. Infiere si una derrota fue "merecida" (mal rendimiento) o "injusta" (buen rendimiento, mala suerte/definición). El texto resultante está diseñado para dar una visión cualitativa más allá de los números.

#### 2.5. Archivo: `modules/analisis_reciente.py` (Análisis de Forma Reciente)

- `analizar_rendimiento_reciente_con_handicap(soup, ...)`
    - **Propósito:** Analiza los últimos 5 partidos de un equipo para ver su rendimiento cubriendo el hándicap.
    - **Extracción HTML:** Itera sobre las filas de la tabla de historial (`table_v1` o `table_v2`) y extrae el resultado y la línea de hándicap de cada uno de los últimos 5 partidos.
    - **Lógica:** Para cada partido, llama a `check_handicap_cover` para ver el resultado y cuenta cuántas veces cubrió, no cubrió o fue push.

- `comparar_lineas_handicap_recientes(soup, ...)`
    - **Propósito:** Compara la línea de hándicap del partido actual con el promedio de las líneas de los últimos partidos.
    - **Lógica:** Calcula el promedio de las líneas de hándicap de los partidos recientes y lo compara con la línea actual para determinar si el mercado ahora ve al equipo como más o menos favorito que antes ("Línea SUBIÓ", "Línea BAJÓ").

#### 2.6. Archivo: `modules/analisis_rivales.py` (Análisis de Rivales)

- `analizar_rivales_comunes(soup, team_a, team_b)`
    - **Propósito:** Encuentra rivales a los que ambos equipos se han enfrentado recientemente y recopila los resultados de dichos partidos.
    - **Extracción HTML:**
        1.  Recorre la tabla de historial del `equipo_a` (`table_v1`) y crea un conjunto con todos sus oponentes.
        2.  Hace lo mismo para el `equipo_b` (`table_v2`).
        3.  Calcula la intersección de ambos conjuntos para encontrar los rivales comunes.
        4.  Vuelve a recorrer las tablas para extraer los detalles completos de los partidos contra esos rivales comunes.

- `analizar_contra_rival_del_rival(soup, ...)`
    - **Propósito:** Implementa la lógica de "comparativa indirecta". Busca cómo le fue al `equipo_a` contra el último rival del `equipo_b`, y viceversa.
    - **Extracción HTML:** Similar a la anterior, recorre las tablas de historial buscando partidos específicos que coincidan con los equipos y rivales cruzados.

#### 2.7. Archivo: `modules/funciones_resumen.py` y `modules/funciones_auxiliares.py`

Estos módulos contienen funciones que combinan la extracción y el análisis para generar resúmenes específicos.

- `generar_resumen_rendimiento_reciente(soup, ...)` en `funciones_resumen.py`:
    - **Propósito:** Orquesta la llamada a otras funciones de análisis para crear un resumen completo del rendimiento reciente, incluyendo la tendencia de hándicap y las comparativas.

- Funciones en `funciones_auxiliares.py` (ej. `_calcular_estadisticas_contra_rival`, `_analizar_desempeno_casa_fuera`):
    - **Propósito:** Son funciones de cálculo que operan sobre listas de partidos ya extraídos. No realizan scraping por sí mismas, sino que procesan los datos para calcular porcentajes de victoria, promedios de goles, etc., que luego se muestran en la interfaz.

--- 

### 3. CONCLUSIÓN FINAL

Este informe final, que abarca todos los módulos del proyecto, confirma la robustez y complejidad de la aplicación. La arquitectura está bien definida, permitiendo análisis de datos multifacéticos a partir de una estrategia de scraping híbrida y eficiente. Cada módulo tiene una responsabilidad clara, desde la extracción de datos brutos hasta la generación de análisis cualitativos, lo que demuestra un diseño de software maduro y bien planificado.
